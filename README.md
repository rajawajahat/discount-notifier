# ğŸ”¥ Fashion Discount Notifier

A scalable, modular web scraping service that monitors men's fashion sales across premium UK retailers and sends Discord notifications when products have high discounts (â‰¥70%).

## ğŸš€ Features

- **ğŸª Multi-Retailer Support**: Monitors Harvey Nichols, Flannels, Selfridges, Harrods, and END Clothing
- **ğŸ”” Smart Notifications**: Only alerts on products with â‰¥70% discount via Discord webhooks
- **âš™ï¸ Modular Architecture**: Easy to add new retailers with minimal code changes
- **ğŸ¤– Selenium Fallback**: Automatic browser automation for anti-bot protection
- **ğŸ“Š Comprehensive Logging**: Detailed logging and error handling
- **ğŸ’ª Robust**: Built-in retry logic, rate limiting, and health checks
- **ğŸ³ Production Ready**: Docker, systemd, and supervisor deployment options

## ğŸ“‹ Requirements

- Python 3.11+
- Discord webhook URL
- Internet connection for scraping
- Chrome browser (for Selenium fallback)

## ğŸš€ Quick Start

### Option 1: Simple Execution (Recommended)

```bash
# Clone the repository
git clone <repository-url>
cd discount-notifier

# Install dependencies
pip install -r requirements.txt

# Set your Discord webhook
export DISCORD_WEBHOOK_URL="your-discord-webhook-url-here"

# Run once (development)
python run_scrapers.py --dev

# Run once (production)
python run_scrapers.py
```

### Option 2: VPS/Server Deployment

```bash
# Run the automated installer (Ubuntu/Debian/CentOS)
sudo bash deploy/install.sh

# The installer will:
# - Install system dependencies
# - Create service user
# - Setup virtual environment
# - Configure systemd service
# - Start the service automatically
```

## ğŸ¯ Usage

### Basic Commands

```bash
# Run all scrapers once (development webhook)
python run_scrapers.py --dev

# Run all scrapers once (production webhook)
python run_scrapers.py

# The script will:
# - Run all 5 scrapers (Flannels, Harrods, Harvey Nichols, Selfridges, END Clothing)
# - Send Discord notifications for products with â‰¥70% discount
# - Show detailed progress and results
```

### Webhook Configuration

The script automatically uses the appropriate webhook:

- **Development**: `--dev` flag uses testing webhook
- **Production**: Default uses production webhook
- **Custom**: Set `DISCORD_WEBHOOK_URL` environment variable

## ğŸª Supported Retailers

| Retailer | URL | Method | Status |
|----------|-----|--------|--------|
| Harvey Nichols | `https://www.harveynichols.com/sale/mens/` | API | âœ… Active |
| Flannels | `https://www.flannels.com/clearance/men/clothing` | API | âœ… Active |
| Selfridges | `https://www.selfridges.com/GB/en/cat/mens/on_sale/` | Selenium | âœ… Active |
| Harrods | `https://www.harrods.com/en-gb/sale/men` | HTML + JSON-LD | âœ… Active |
| END Clothing | `https://www.endclothing.com/gb/sale` | HTML | âœ… Active |

## ğŸ”” Discord Notifications

The system sends rich Discord notifications for high-discount products including:

- ğŸ”¥ Product name with discount percentage
- ğŸ’° Original and sale prices
- ğŸ“Š Discount amount and percentage
- ğŸª Retailer information
- ğŸ• Discovery timestamp
- ğŸ–¼ï¸ Product image (when available)
- ğŸ”— Direct link to product

## ğŸ—ï¸ Architecture

### Project Structure

```
discount-notifier/
â”œâ”€â”€ scrapers/                    # Individual retailer scrapers
â”‚   â”œâ”€â”€ base.py                 # Abstract scraper interface
â”‚   â”œâ”€â”€ flannels.py             # Flannels API scraper
â”‚   â”œâ”€â”€ harrods.py              # Harrods HTML + JSON-LD scraper
â”‚   â”œâ”€â”€ harvey_nichols.py       # Harvey Nichols API scraper
â”‚   â”œâ”€â”€ selfridges.py           # Selfridges Selenium scraper
â”‚   â””â”€â”€ end_clothing.py         # END Clothing HTML scraper
â”œâ”€â”€ notifications/
â”‚   â””â”€â”€ discord_notifier.py     # Discord webhook handler
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logging_setup.py        # Logging configuration
â”œâ”€â”€ deploy/                     # Deployment configurations
â”‚   â”œâ”€â”€ install.sh              # Automated installer
â”‚   â”œâ”€â”€ systemd/                # Systemd service files
â”‚   â””â”€â”€ supervisor/             # Supervisor configuration
â”œâ”€â”€ run_scrapers.py             # Main execution script
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

### Adding New Retailers

1. Create a new scraper class inheriting from `BaseScraper`:

```python
from scrapers.base import BaseScraper, Product

class NewRetailerScraper(BaseScraper):
    def __init__(self):
        super().__init__("New Retailer", "https://example.com/sale")
    
    def scrape_products(self) -> List[Product]:
        # Implementation here
        pass
    
    def parse_product_data(self, element) -> Optional[Product]:
        # Implementation here
        pass
```

2. Add it to `run_scrapers.py`:

```python
from scrapers.new_retailer import NewRetailerScraper

# In the scrapers list:
scrapers = [
    (FlannelsScraper, "Flannels"),
    (HarrodsScraper, "Harrods"),
    (HarveyNicholsScraper, "Harvey Nichols"),
    (SelfridgesScraper, "Selfridges"),
    (EndClothingScraper, "END Clothing"),
    (NewRetailerScraper, "New Retailer")  # Add here
]
```

## ğŸ¤– Selenium Deployment

The Selfridges scraper includes automatic Selenium fallback for anti-bot protection. This section covers cloud deployment requirements.

### Local Development

```bash
# Install dependencies
pip install selenium webdriver-manager

# Test the scraper
python run_scrapers.py --dev
```

### Cloud Deployment Requirements

#### VPS/Dedicated Server (Ubuntu/Debian)

```bash
# Update system
sudo apt-get update

# Install Chrome browser
wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
sudo apt-get update
sudo apt-get install -y google-chrome-stable

# Install Xvfb (virtual display for headless)
sudo apt-get install -y xvfb

# Set up virtual display
export DISPLAY=:99
Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
```

#### AWS EC2 Setup Script

```bash
#!/bin/bash
# Run this on a fresh Ubuntu EC2 instance

# Update system
sudo apt-get update -y

# Install Python and pip
sudo apt-get install -y python3 python3-pip git

# Install Chrome dependencies
wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
sudo apt-get update -y
sudo apt-get install -y google-chrome-stable xvfb

# Clone and setup application
git clone <your-repo-url>
cd discount-notifier
pip3 install -r requirements.txt

# Set up systemd service
sudo tee /etc/systemd/system/discount-notifier.service > /dev/null <<EOF
[Unit]
Description=Fashion Discount Notifier
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/home/ubuntu/discount-notifier
Environment=DISPLAY=:99
Environment=DISCORD_WEBHOOK_URL=your_webhook_url_here
ExecStartPre=/usr/bin/Xvfb :99 -screen 0 1024x768x24 -ac +extension GLX +render -noreset
ExecStart=/usr/bin/python3 run_scrapers.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# Enable and start service
sudo systemctl enable discount-notifier
sudo systemctl start discount-notifier
```

### Selenium Configuration

#### Environment Variables

```bash
# Required
export DISCORD_WEBHOOK_URL="your_webhook_url"

# Optional Selenium settings
export SELENIUM_HEADLESS=true          # Run browser in headless mode (default: true)
export SELENIUM_TIMEOUT=30             # Page load timeout in seconds (default: 30)
export SELENIUM_MAX_PAGES=5            # Max pages to scrape with Selenium (default: 5)
export DISPLAY=:99                     # Virtual display (required for headless)
```

#### Chrome Options (Automatically Configured)

The scraper automatically uses these Chrome options for cloud compatibility:
- `--headless` - Run without GUI
- `--no-sandbox` - Required for Docker/cloud
- `--disable-dev-shm-usage` - Prevents crashes
- `--disable-gpu` - Better performance
- Anti-detection options

## ğŸ³ Deployment Options

### Systemd Service

```bash
# Install via script
sudo bash deploy/install.sh

# Manual service management
sudo systemctl start fashion-discount-notifier
sudo systemctl status fashion-discount-notifier
sudo journalctl -u fashion-discount-notifier -f
```

### Supervisor

```bash
# Copy configuration
sudo cp deploy/supervisor/fashion-discount-notifier.conf /etc/supervisor/conf.d/
sudo supervisorctl reread
sudo supervisorctl update
sudo supervisorctl start fashion-discount-notifier
```

### Cron Jobs

```bash
# Add to crontab for hourly execution
crontab -e

# Add this line:
0 * * * * cd /path/to/discount-notifier && python run_scrapers.py
```

## ğŸ“Š Performance Considerations

| Method | Speed | Success Rate | Resource Usage |
|--------|-------|--------------|----------------|
| Regular Requests | ~1-2 seconds | 0% (blocked) | Very Low |
| Selenium Fallback | ~15-30 seconds | 85-95% | Medium-High |

### Memory Requirements
- **Minimum**: 512MB RAM
- **Recommended**: 1GB+ RAM
- **Chrome browser**: ~200-400MB per instance

### CPU Usage
- **Selenium**: ~20-50% CPU during scraping
- **Regular requests**: <5% CPU

## ğŸš¨ Troubleshooting

### Common Issues

#### 1. Discord Webhook Not Working
```bash
# Verify webhook URL is correct
# Test with: python run_scrapers.py --dev
```

#### 2. Selenium Display Errors
```bash
# Error: "cannot connect to X server"
# Solution: Ensure Xvfb is running
export DISPLAY=:99
Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
```

#### 3. Chrome Permission Errors
```bash
# Error: "Chrome didn't start correctly"
# Solution: Add --no-sandbox flag (already included)
```

#### 4. Memory Issues
```bash
# Error: "Chrome crashed"
# Solution: Increase available memory or add swap
sudo fallocate -l 1G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### Debug Mode

To enable verbose logging:

```bash
# Set environment variable
export LOG_LEVEL=DEBUG

# Run with debug output
python run_scrapers.py --dev
```

## ğŸ“Š Monitoring

### Health Checks

```bash
# Check if Chrome is accessible
google-chrome --version

# Check virtual display
ps aux | grep Xvfb

# Test scraper
python run_scrapers.py --dev
```

### Logs

Monitor these log files:
- Application logs: `logs/discount_notifier.log`
- System logs: `sudo journalctl -u fashion-discount-notifier -f`

## ğŸ¯ Success Indicators

When the system is working correctly, you'll see:

```
ğŸš€ Discount Notifier - Full Scraping Run
============================================================
â° Started at: 14:30:15 on 15/01/2025

ğŸ” Running Flannels
============================================================
âœ… Flannels: Completed in 12.3s
ğŸ“¦ Flannels: Found 3 high discount products

ğŸ” Running Selfridges
============================================================
ğŸ¤– Selenium: Browser initialized successfully
ğŸ¤– Selenium Page 1: Loading https://www.selfridges.com/...
âœ… Selfridges: Completed in 45.2s
ğŸ“¦ Selfridges: Found 0 high discount products

ğŸ“Š FINAL SUMMARY
============================================================
âœ… Successful scrapers: 5/5
âŒ Failed scrapers: 0
ğŸ“¦ Total high discount products: 3
â° Total runtime: 120.5s
```

## ğŸ› ï¸ Development

### Setup Development Environment

```bash
# Clone repository
git clone <repository-url>
cd discount-notifier

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

### Testing Individual Scrapers

```bash
# Test single scraper
python -c "
from scrapers.flannels import FlannelsScraper
scraper = FlannelsScraper()
products = scraper.scrape_products()
print(f'Found {len(products)} products')
"

# Test Discord notifications
python -c "
from notifications.discord_notifier import DiscordNotifier
notifier = DiscordNotifier('your-webhook-url')
notifier.test_webhook()
"
```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ Support

- Create an issue for bugs or feature requests
- Check logs for troubleshooting
- Use `python run_scrapers.py --dev` for testing

---

**âš ï¸ Disclaimer**: This tool is for educational purposes. Please respect robots.txt and terms of service of the scraped websites. Use reasonable scraping intervals to avoid overloading servers.